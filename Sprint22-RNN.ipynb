{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リカレントニューラルネットワークスクラッチ\n",
    "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$\n",
    "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n",
    "h_t = tanh(a_t)\n",
    "$$\n",
    "\n",
    "\n",
    "at : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "ht : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "xt : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "\n",
    "Wx : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "\n",
    "ht-1 : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "Wh : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "\n",
    "B : バイアス項 (n_nodes,)\n",
    "\n",
    "初期状態 h0は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力xは(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:49:16.016698Z",
     "start_time": "2021-01-17T06:49:16.014607Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:51:08.352368Z",
     "start_time": "2021-01-17T06:51:08.345411Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleRNN():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # 【問題1】\n",
    "        self.w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "        self.w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "        self.b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "        \n",
    "        #n_nodes = self.w_x.shape[1] # 4\n",
    "        #self.h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "        \n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        #self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        #self.B = initializer.B(n_nodes2)\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #【問題２】\n",
    "        self.x = x\n",
    "        \n",
    "        #入力xと前の層の重みht−１に重みをかけて足す\n",
    "        A = self.x@self.w_x + h_prev@self.w_h + self.b.T#.reshape(-1,1)\n",
    "        h_next = np.tanh(A)# 次の計算で使う（コンテクスト）\n",
    "        \n",
    "        self.A = A\n",
    "        self.H = h_next\n",
    "        \n",
    "        return h_next\n",
    "    \n",
    "    # 【問題３】\n",
    "    def backward(self, dH):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 更新\n",
    "        self, dX, dH_prev = self.optimizer.update(self, dH)\n",
    "        \n",
    "        return (dX, dH_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:52:03.142734Z",
     "start_time": "2021-01-17T06:52:03.139324Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        H = np.tanh(A)\n",
    "        \n",
    "        return H\n",
    "    \n",
    "\n",
    "    def backward(self, X, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        dA = dZ*(1 - np.tanh(X)**2)\n",
    "        \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:52:24.442974Z",
     "start_time": "2021-01-17T06:52:24.438572Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer():\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        #【問題2】\n",
    "        B = self.sigma * np.random.randn(n_nodes2,)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:52:37.198566Z",
     "start_time": "2021-01-17T06:52:37.193632Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, dH):#Z3, Y, \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        #【問題3】\n",
    "        \n",
    "        dAt = dH*(1 - np.tanh(layer.A)**2)\n",
    "        \n",
    "        dWx = layer.x.reshape(-1,1)@dAt\n",
    "        dWh = layer.H.T@dAt\n",
    "        \n",
    "        layer.w_x = layer.w_x - self.lr*(dWx)\n",
    "        layer.w_h = layer.w_h - self.lr*(dWh)\n",
    "        layer.b = layer.b - self.lr*(dAt)\n",
    "        \n",
    "        dX = dAt@layer.w_x.T\n",
    "        dH_prev = dAt@layer.w_h.T\n",
    "        \n",
    "        return (layer, dX, dH_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:52:49.485722Z",
     "start_time": "2021-01-17T06:52:49.481231Z"
    }
   },
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\"\n",
    "    活性化関数（シグモイド関数）のクラス\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        準伝播用\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        Z = np.exp(X)/(np.sum(np.exp(X), axis=1).reshape(-1,1))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        逆伝播用\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        \"\"\"\n",
    "        dA =X - y\n",
    "        \n",
    "        # 目的関数（損失関数）　交差エントロピー誤差\n",
    "        nb = y.shape[0]#バッチサイズ\n",
    "        L = -(1/nb)*(np.sum(y*np.log(X)))\n",
    "        \n",
    "        return (dA, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:53:09.397139Z",
     "start_time": "2021-01-17T06:53:09.389682Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScratchSimpleRNNClassifier():\n",
    "    \"\"\"\n",
    "    RNNのクラス\n",
    "    Parameters\n",
    "    ----------\n",
    "    self.n_features \n",
    "    \n",
    "    self.n_nodes1\n",
    "    \n",
    "    self.sigma\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, lr, sigma = 0.01):\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self, x, epoch):\n",
    "        #initializer = SimpleInitializer(n_nodes)\n",
    "        initializer = SimpleInitializer(1)\n",
    "        optimizer = SGD(self.lr)\n",
    "        batch_size = x.shape[0] # 1\n",
    "\n",
    "        \n",
    "        self.RNN = SimpleRNN(batch_size, initializer, optimizer)\n",
    "        self.activation = Softmax()\n",
    "        \n",
    "        \n",
    "        n_nodes = 4#self.w_x.shape[1] # 4\n",
    "        h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "        \n",
    "        \n",
    "        for i in range(epoch):\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                # フォワード処理\n",
    "                H1 = self.RNN.forward(x[0, 0], h)\n",
    "                y1= self.activation.forward(H1)\n",
    "                H2 = self.RNN.forward(x[0, 1], H1)\n",
    "                y2= self.activation.forward(H2)\n",
    "                self.H3 = self.RNN.forward(x[0, 2], H2)\n",
    "                y3= self.activation.forward(H3)\n",
    "\n",
    "                # 問題３\n",
    "                # バックワード処理\n",
    "                self.dX2, dH2 = self.RNN.backward(H3)\n",
    "                self.dX1, dH1= self.RNN.backward(dH2)\n",
    "                self.dX0, self.dH0 = self.RNN.backward(dH1)\n",
    "                \n",
    "        \n",
    "#    def predict(self,X):\n",
    "#\n",
    "#        # 最初のZはXなので代入\n",
    "#        Z = X\n",
    "#\n",
    "#        # フォワード処理\n",
    "#        for l in range(self.layer):\n",
    "#            A = self.FC_list[l].forward(Z)\n",
    "#            Z = self.activation_list[l].forward(A)\n",
    "#            \n",
    "#        y = self.activation_list[-1].forward(A)\n",
    "#        pred = np.argmax(y, axis=1)\n",
    "#        \n",
    "#        #A1 = self.FC_list[0].forward(X)\n",
    "#        #Z1 = self.activation_list[0].forward(A1)\n",
    "#        #A2 = self.FC_list[1].forward(Z1)\n",
    "#        #Z2 = self.activation_list[1].forward(A2)\n",
    "#        #A3 = self.FC_list[2].forward(Z2)\n",
    "#        #y = self.activation_list[2].forward(A3)\n",
    "#        #pred = np.argmax(y, axis=1)\n",
    "#        \n",
    "#        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:54:39.599824Z",
     "start_time": "2021-01-17T06:54:39.595670Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "#w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "#w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "#n_features = x.shape[2] # 2\n",
    "#n_nodes = w_x.shape[1] # 4\n",
    "n_nodes = 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "#b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:54:58.980214Z",
     "start_time": "2021-01-17T06:54:58.976441Z"
    }
   },
   "outputs": [],
   "source": [
    "#RNN層\n",
    "lr = 0.001\n",
    "#initializer = SimpleInitializer(n_nodes)\n",
    "initializer = SimpleInitializer(1)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "batch_size = 1#x.shape[0] # 1\n",
    "\n",
    "RNN = SimpleRNN(batch_size, initializer, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:55:17.454598Z",
     "start_time": "2021-01-17T06:55:17.452220Z"
    }
   },
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "#activation1 = Tanh()\n",
    "#activation2 = Tanh()\n",
    "#activation3 = Tanh()\n",
    "activation = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:55:49.770994Z",
     "start_time": "2021-01-17T06:55:49.765853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "# フォワードプロパゲーション\n",
    "\n",
    "H1 = RNN.forward(x[0,0], h)\n",
    "y1= activation.forward(H1)\n",
    "H2 = RNN.forward(x[0,1], H1)\n",
    "y2= activation.forward(H2)\n",
    "H3 = RNN.forward(x[0,2], H2)\n",
    "y3= activation.forward(H3)\n",
    "\n",
    "print(H3)\n",
    "\n",
    "# DIVERに記載された正解\n",
    "# h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T06:56:10.005525Z",
     "start_time": "2021-01-17T06:56:10.001677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h [[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "# クラスから実行\n",
    "RNN_class = ScratchSimpleRNNClassifier(lr=0.001, sigma = 0.01)\n",
    "RNN_class.fit(x, 1)\n",
    "print(\"h\", RNN_class.H3)\n",
    "#print(\"Z1\", RNN_class.Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】（アドバンス課題）バックプロパゲーションの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
    "\n",
    "$$\n",
    "W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
    "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
    "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n",
    "$$\n",
    "\n",
    "$$\n",
    "α : 学習率\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x}: \n",
    "W\n",
    "x\n",
    " に関する損失 \n",
    "L\n",
    " の勾配\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_h}: \n",
    "W\n",
    "h\n",
    " に関する損失 \n",
    "L\n",
    " の勾配\n",
    "$$\n",
    " \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial B}:B\n",
    "  に関する損失 \n",
    "L\n",
    " の勾配\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial B} = \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "*\\frac{\\partial L}{\\partial h_t}は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前の時刻や層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScratchSimpleRNNClassifierクラスfit関数内に以下のコードを作成。\n",
    "\n",
    "dX2, dH2 = self.RNN.backward(H3)\n",
    "\n",
    "dX1, dH1= self.RNN.backward(dH2)\n",
    "\n",
    "dX0, dH0 = self.RNN.backward(dH1)\n",
    "\n",
    "SGD()クラス内にupdate関数を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T07:27:35.996312Z",
     "start_time": "2021-01-17T07:27:35.991309Z"
    }
   },
   "outputs": [],
   "source": [
    "def update(self, layer, dH):#Z3, Y, \n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        #【問題3】\n",
    "        \n",
    "        dAt = dH*(1 - np.tanh(layer.A)**2)\n",
    "        \n",
    "        dWx = layer.x.reshape(-1,1)@dAt\n",
    "        dWh = layer.H.T@dAt\n",
    "        \n",
    "        layer.w_x = layer.w_x - self.lr*(dWx)\n",
    "        layer.w_h = layer.w_h - self.lr*(dWh)\n",
    "        layer.b = layer.b - self.lr*(dAt)\n",
    "        \n",
    "        dX = dAt@layer.w_x.T\n",
    "        dH_prev = dAt@layer.w_h.T\n",
    "        \n",
    "        return (layer, dX, dH_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T07:27:59.187411Z",
     "start_time": "2021-01-17T07:27:59.181265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dX2 [[0.03945182 0.05795526]]\n",
      "dX1 [[0.00280725 0.0039533 ]]\n",
      "dX0 [[0.00019248 0.00027198]]\n",
      "dH0 [[0.0001915  0.00023764 0.00027096 0.00032993]]\n"
     ]
    }
   ],
   "source": [
    "# バックワードプロパゲーション\n",
    "\n",
    "dX2, dH2 = RNN.backward(H3)\n",
    "dX1, dH1= RNN.backward(dH2)\n",
    "dX0, dH0 = RNN.backward(dH1)\n",
    "\n",
    "print(\"dX2\", dX2)\n",
    "print(\"dX1\", dX1)\n",
    "print(\"dX0\", dX0)\n",
    "print(\"dH0\", dH0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T07:28:09.734959Z",
     "start_time": "2021-01-17T07:28:09.730258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 [[0.03945182 0.05795526]]\n",
      "x1 [[0.00280725 0.0039533 ]]\n",
      "x0 [[0.00019248 0.00027198]]\n",
      "dH0 [[0.0001915  0.00023764 0.00027096 0.00032993]]\n"
     ]
    }
   ],
   "source": [
    "# クラスから実行\n",
    "RNN_class = ScratchSimpleRNNClassifier(lr=0.001, sigma = 0.01)\n",
    "RNN_class.fit(x, 1)\n",
    "print(\"x2\", RNN_class.dX2)\n",
    "print(\"x1\", RNN_class.dX1)\n",
    "print(\"x0\", RNN_class.dX0)\n",
    "print(\"dH0\", RNN_class.dH0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
